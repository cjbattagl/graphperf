\documentclass[11pt]{article}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{todonotes}
\usepackage{url}
\usepackage{xcolor}

\setlength{\paperwidth}{8.5in}
\setlength{\paperheight}{11in}
\setlength{\voffset}{-0.2in}
\setlength{\topmargin}{0in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\footskip}{30pt}
\setlength{\textheight}{9.25in}
\setlength{\hoffset}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\parindent}{0in}
\setlength{\parskip}{9pt}

\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}


\newcommand\warning[1]{\textcolor{red}{#1}}

\DeclareGraphicsRule{.JPG}{eps}{*}{`jpeg2ps #1}

\title{Research Review}
\author{Casey Battaglino}
\date{}
\begin{document}
\maketitle

\section{Introduction}

I have structured this document as follows: each section presents a particular topic I've been exploring. I begin with an overview that covers some of the basic work that's been done on the subject. Then I have a `thoughts' section that introduces some of my thoughts on the matter. Then I have an `analysis' section that discusses any original experiments, tests, visualizations, and implementations that I've done related to the topic. 

\section{Distributed-Memory (Re-)Streaming Graph Partitioning}

\subsection{Overview}

Offline graph partitioning algorithms, where the graph is allowed to exist in memory with total information about edges, have existed for decades. Hundreds of variants of such algorithms now exist, and range from spatial methods~\cite{Gilbert95geometricmesh} to spectral methods~\cite{arora2009expander}. The most scalable and effective graph partitioners on massive datasets are currently multi-level partitioners, which recursively contract the graph to a small number of vertices, and then heuristically optimize the problem on each subsequent expansion~\cite{karypis1998multilevel}. 

Streaming graph partitioning has only been introduced in the last couple of years~\cite{DBLP:journals/corr/abs-1212-1121,Stanton:2012:SGP:2339530.2339722,tsourakakis2012fennel}, as graphs grow to the point where they may not fit into memory, or must be distributed to compute nodes on the fly. In the streaming model, input data (vertices) arrive sequentially from a generating source (such as a web-crawler), and must be partitioned as they arrive.

The generated partitions are dependent on the order in which vertices are processed. For instance, a web crawler might generate vertices in an order that represents a Breadth-First or Depth-First traversal of the web, or we may receive vertices in a random order. An analysis of streaming algorithms may also consider an adversarial ordering that produces the worst possible results~\cite{Stanton:2012:SGP:2339530.2339722}.

Assuming vertices arrive in some order, a heuristic makes a partition decision, given vertex $v$, and capacity constraint $C$ (where $C$ is generally $\approx \frac{(\epsilon+|V|)}{n}$ Stanton explores a number of heuristics (from simple to complex), and finds that a simple weighted greedy approach is most effective ~\cite{Stanton:2012:SGP:2339530.2339722}. This heuristic is as follows:

\textbf{Weighted Deterministic Greedy (WDG):} Assign $v$ to partition that it has most edges to, weighted by the relative size of each partition (weight function can be linear or exponential).

\subsection{Thoughts}
This heuristic proves surprisingly effective for scale-free graphs, and can be competitive with leading offline implementations such as METIS, while operating at a much faster speed. The low diameter of scale-free graphs seems to make them easier to partition in this way (because as we process more and more nodes, the chance that an incoming node has neighbors that have already been partitioned becomes very high. This is not true for spatial, structured graphs). 

An initial idea that Robert and I came up with was to \textit{re-stream} the partitioning: first, we do a single pass over the nodes, and then we do a second pass, re-assigning nodes to partitions that they have more neighbors to. (Alternatively, we can reassign nodes any time we encounter them in our algorithm). This turned out to be remarkably convergent and effective: after 4 passes, we had improved partition quality by a factor of 2-3. Unfortunately, I discovered a paper that implemented this, after we had already done our own implementation~\cite{Nishiura13}.

\warning{Make graph of node-degree cutoff vs partition quality}

\warning{Show re-streaming quality increase}

\subsection{Implementation}

\section{Probabilistic Study of Graph Traversals}


\section{Distributed Scale-Free Graph Traversals}
Goal: a communication pattern that optimizes distributed graph traversals.

\section{Other Open Questions}

\warning{Approximate graph algorithms (sampling) -- power-reduction}




\bibliographystyle{plain}
\bibliography{bib}


\end{document}